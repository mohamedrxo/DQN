{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "bb79c394",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb79c394",
        "outputId": "637c3a14-4fdc-44c0-8039-c97966d6d26a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reward -1.0\n",
            "reward -1.0\n",
            "reward -1.0\n",
            "reward -1.0\n",
            "reward -1.0\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import ale_py\n",
        "import numpy as np\n",
        "\n",
        "# Custom wrapper to give -1 reward on life loss\n",
        "class NegativeLifeRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.lives = 0\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        # ALE keeps track of lives\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        # Check current lives\n",
        "        current_lives = self.env.unwrapped.ale.lives()\n",
        "        # If a life is lost, give negative reward\n",
        "        if current_lives < self.lives:\n",
        "            reward -= 1.0  # penalty for losing a life\n",
        "            self.lives = current_lives\n",
        "\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "\n",
        "# Usage\n",
        "gym.register_envs(ale_py)\n",
        "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"human\")\n",
        "env = NegativeLifeRewardWrapper(env)\n",
        "\n",
        "observation, info = env.reset()\n",
        "episode_over = False\n",
        "\n",
        "while not episode_over:\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    if reward != 0.0:\n",
        "        print(\"reward\", reward)\n",
        "    episode_over = terminated or truncated\n",
        "\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6d3e944a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "6d3e944a",
        "outputId": "a2384649-4ff2-4bde-a351-480bccedf41b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(84, 84)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ca53f7290d0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def preprocess_image(frame,new_size=(84, 84)):\n",
        "        \"\"\"\n",
        "        Takes an Atari Breakout RGB frame (210x160x3)\n",
        "        Returns a cropped, grayscale, resized frame (H, W)\n",
        "        \"\"\"\n",
        "        # 1. Convert to grayscale\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        # 2. Crop the game area (remove top score + borders)\n",
        "        # Breakout: useful area is row 34 to 194\n",
        "        cropped = gray[34:194, :]   # shape: (160,160)\n",
        "\n",
        "        # 3. Resize to new_size\n",
        "        resized = cv2.resize(cropped, new_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        # 4. Normalize (optional)\n",
        "        normalized = resized.astype(np.float32) / 255.0\n",
        "\n",
        "        return normalized\n",
        "\n",
        "\n",
        "image = preprocess_image(observation)\n",
        "print(image.shape)\n",
        "plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "53c450a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53c450a3",
        "outputId": "f3f5e290-90be-485c-b003-b9e5b9c39c4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "ENV_NAME = \"ALE/Breakout-v5\"\n",
        "LEARNING_RATE = 1e-3\n",
        "EPISODES = 50000\n",
        "GAMMA = 0.99\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_END = 0.00\n",
        "EPSILON_DECAY = 0.99\n",
        "BATCH_SIZE = 3000\n",
        "MEMORY_SIZE = 1000000\n",
        "TARGET_UPDATE = 10\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "# Neural Network for Q-Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_channels, action_dim):\n",
        "        \"\"\"\n",
        "        input_channels: number of stacked frames (usually 4)\n",
        "        action_dim: number of actions in Breakout (4 for Gym)\n",
        "        \"\"\"\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),   # (4, 84, 84) -> (32, 20, 20)\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),               # (32, 20, 20) -> (64, 9, 9)\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),               # (64, 9, 9) -> (64, 7, 7)\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * 7 * 7, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: shape (batch, 4, 84, 84)\n",
        "        \"\"\"\n",
        "        x = x.to(device)        # ensure input is on same device\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)   # flatten\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ca09a1fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca09a1fb",
        "outputId": "a97e7852-499b-461e-d225-957272701ff3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(4)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model = DQN(4,env.env.action_space.n).to(device)\n",
        "\n",
        "input = torch.randn((1,4,84,84))\n",
        "\n",
        "out = model(input)\n",
        "\n",
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "19e4279e",
      "metadata": {
        "id": "19e4279e"
      },
      "outputs": [],
      "source": [
        "from collections import deque , namedtuple\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "e87f760c",
      "metadata": {
        "id": "e87f760c"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward','done'))\n",
        "\n",
        "#the replayMemory Class\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "2f2d83de",
      "metadata": {
        "id": "2f2d83de"
      },
      "outputs": [],
      "source": [
        "def select_action(policy_net,state):\n",
        "    if random.random()<epsilon:\n",
        "         action = env.action_space.sample()\n",
        "         return action\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "\n",
        "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "            return policy_net(state).argmax(dim=1).item()\n",
        "\n",
        "def train_dqn(policy_net,target_net,memory,optimizer):\n",
        "    if memory.__len__()<BATCH_SIZE:\n",
        "        return\n",
        "    sample = memory.sample(BATCH_SIZE)\n",
        "\n",
        "\n",
        "    batch = Transition(*zip(*sample))\n",
        "\n",
        "    # Convert all batch elements to single NumPy arrays first\n",
        "    states = torch.tensor(np.array(batch.state), dtype=torch.float32).to(device)\n",
        "    actions = torch.tensor(np.array(batch.action), dtype=torch.int64).unsqueeze(1).to(device)\n",
        "    next_states = torch.tensor(np.array(batch.next_state), dtype=torch.float32).to(device)\n",
        "    rewards = torch.tensor(np.array(batch.reward), dtype=torch.float32).to(device)\n",
        "    dones = torch.tensor(np.array(batch.done), dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "    current_q_values = policy_net(states).gather(1,actions)\n",
        "\n",
        "    next_q_values =  target_net(next_states).max(1)[0].detach()\n",
        "    target_q_values = rewards +(GAMMA * next_q_values * (1 - dones))\n",
        "\n",
        "    loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n",
        "\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3bed584",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3bed584",
        "outputId": "69964454-2195-4740-aee0-dba837210e56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reward:  -1.0 epsilon 0.78\n",
            "reward:  -2.0 epsilon 0.78\n",
            "reward:  -3.0 epsilon 0.78\n",
            "reward:  -3.0 epsilon 0.78\n",
            "Episode 1, Reward: -4.0, Epsilon: 0.77\n",
            "reward:  1.0 epsilon 0.77\n",
            "reward:  -1.0 epsilon 0.77\n",
            "reward:  -2.0 epsilon 0.77\n",
            "reward:  -3.0 epsilon 0.77\n",
            "Episode 2, Reward: -3.0, Epsilon: 0.76\n",
            "reward:  -1.0 epsilon 0.76\n",
            "reward:  -1.0 epsilon 0.76\n",
            "reward:  0.0 epsilon 0.76\n",
            "reward:  -1.0 epsilon 0.76\n",
            "reward:  -1.0 epsilon 0.76\n",
            "reward:  -1.0 epsilon 0.76\n",
            "Episode 3, Reward: -2.0, Epsilon: 0.75\n",
            "reward:  0.0 epsilon 0.75\n",
            "reward:  0.0 epsilon 0.75\n",
            "reward:  1.0 epsilon 0.75\n",
            "reward:  1.0 epsilon 0.75\n",
            "reward:  0.0 epsilon 0.75\n",
            "reward:  -1.0 epsilon 0.75\n",
            "reward:  0.0 epsilon 0.75\n",
            "Episode 4, Reward: -1.0, Epsilon: 0.75\n",
            "reward:  0.0 epsilon 0.75\n",
            "reward:  -2.0 epsilon 0.75\n",
            "reward:  -4.0 epsilon 0.75\n",
            "reward:  -3.0 epsilon 0.75\n",
            "Episode 5, Reward: -4.0, Epsilon: 0.74\n",
            "reward:  -1.0 epsilon 0.74\n",
            "reward:  -2.0 epsilon 0.74\n",
            "reward:  -2.0 epsilon 0.74\n",
            "reward:  -2.0 epsilon 0.74\n",
            "Episode 6, Reward: -3.0, Epsilon: 0.73\n",
            "reward:  0.0 epsilon 0.73\n",
            "reward:  -1.0 epsilon 0.73\n",
            "reward:  -1.0 epsilon 0.73\n",
            "reward:  -1.0 epsilon 0.73\n",
            "reward:  0.0 epsilon 0.73\n",
            "reward:  0.0 epsilon 0.73\n",
            "reward:  -1.0 epsilon 0.73\n",
            "reward:  0.0 epsilon 0.73\n",
            "Episode 7, Reward: -1.0, Epsilon: 0.72\n",
            "reward:  1.0 epsilon 0.72\n",
            "reward:  1.0 epsilon 0.72\n",
            "reward:  2.0 epsilon 0.72\n",
            "reward:  1.0 epsilon 0.72\n",
            "reward:  1.0 epsilon 0.72\n",
            "reward:  5.0 epsilon 0.72\n",
            "reward:  4.0 epsilon 0.72\n",
            "Episode 8, Reward: 7.0, Epsilon: 0.72\n",
            "reward:  1.0 epsilon 0.72\n",
            "reward:  2.0 epsilon 0.72\n",
            "reward:  3.0 epsilon 0.72\n",
            "reward:  2.0 epsilon 0.72\n",
            "reward:  2.0 epsilon 0.72\n",
            "reward:  0.0 epsilon 0.72\n",
            "Episode 9, Reward: 0.0, Epsilon: 0.71\n",
            "reward:  1.0 epsilon 0.71\n",
            "reward:  0.0 epsilon 0.71\n",
            "reward:  1.0 epsilon 0.71\n",
            "reward:  0.0 epsilon 0.71\n",
            "reward:  0.0 epsilon 0.71\n",
            "reward:  3.0 epsilon 0.71\n",
            "Episode 10, Reward: 2.0, Epsilon: 0.70\n",
            "reward:  -1.0 epsilon 0.70\n",
            "reward:  -1.0 epsilon 0.70\n",
            "reward:  -1.0 epsilon 0.70\n",
            "Saving model...\n",
            "best rewards -3.0\n",
            "Episode 11, Reward: -3.0, Epsilon: 0.70\n",
            "reward:  1.0 epsilon 0.70\n",
            "reward:  -1.0 epsilon 0.70\n",
            "reward:  -1.0 epsilon 0.70\n",
            "reward:  -3.0 epsilon 0.70\n",
            "reward:  -3.0 epsilon 0.70\n",
            "reward:  -1.0 epsilon 0.70\n",
            "Saving model...\n",
            "best rewards -2.0\n",
            "Episode 12, Reward: -2.0, Epsilon: 0.69\n",
            "reward:  -1.0 epsilon 0.69\n",
            "reward:  -1.0 epsilon 0.69\n",
            "reward:  -2.0 epsilon 0.69\n",
            "reward:  -3.0 epsilon 0.69\n",
            "reward:  -4.0 epsilon 0.69\n",
            "Episode 13, Reward: -5.0, Epsilon: 0.68\n",
            "reward:  -1.0 epsilon 0.68\n",
            "reward:  -2.0 epsilon 0.68\n",
            "reward:  -3.0 epsilon 0.68\n",
            "reward:  -4.0 epsilon 0.68\n",
            "Episode 14, Reward: -5.0, Epsilon: 0.68\n",
            "reward:  -1.0 epsilon 0.68\n",
            "reward:  -2.0 epsilon 0.68\n",
            "reward:  -3.0 epsilon 0.68\n",
            "reward:  -4.0 epsilon 0.68\n"
          ]
        }
      ],
      "source": [
        "# LEARNING_RATE = 0.0001\n",
        "# EPISODES = 200\n",
        "\n",
        "gym.register_envs(ale_py)\n",
        "env = gym.make(ENV_NAME)\n",
        "env = NegativeLifeRewardWrapper(env)\n",
        "\n",
        "state_dim = 4\n",
        "action_dim = env.action_space.n\n",
        "trained_model = DQN(state_dim, action_dim)\n",
        "# trained_model.load_state_dict(torch.load(\"breakout.pth\"))\n",
        "\n",
        "policy_net = DQN(state_dim, action_dim).to(device)\n",
        "target_net = DQN(state_dim, action_dim).to(device)\n",
        "\n",
        "target_net.load_state_dict(trained_model.state_dict())\n",
        "policy_net.load_state_dict(trained_model.state_dict())\n",
        "\n",
        "target_net.train()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
        "memory = ReplayMemory(MEMORY_SIZE)\n",
        "\n",
        "rewards = []\n",
        "stacked_current_frames =  deque(maxlen=4)\n",
        "stacked_next_frames =  deque(maxlen=4)\n",
        "\n",
        "frame_skipping = 4\n",
        "best_reward = -float(\"inf\")\n",
        "save_model = False\n",
        "for episode in range(EPISODES):\n",
        "\n",
        "    stacked_current_frames.clear()\n",
        "    stacked_next_frames.clear()\n",
        "\n",
        "    state = env.reset()[0]\n",
        "    episode_reward = 0\n",
        "    last_frame = np.zeros((84,84))\n",
        "    state = preprocess_image(state)\n",
        "    current_frame = state\n",
        "    t=0\n",
        "    while True:\n",
        "\n",
        "        t+=1\n",
        "        stacked_current_frames.append(state)\n",
        "\n",
        "        if t %4 == 0 or t == 0:\n",
        "            if len(stacked_current_frames)==4:\n",
        "                action = select_action( policy_net,stacked_current_frames)\n",
        "                if len(memory) >BATCH_SIZE:\n",
        "                    save_model=True\n",
        "            else:\n",
        "                action= env.action_space.sample()\n",
        "\n",
        "        current_frame  = np.maximum(current_frame,last_frame)\n",
        "\n",
        "        next_state, reward, done, truncated, __= env.step(action)\n",
        "        next_state = preprocess_image(next_state)\n",
        "\n",
        "        stacked_next_frames = stacked_current_frames.copy()\n",
        "        stacked_next_frames.append(next_state)\n",
        "\n",
        "        if len(stacked_current_frames)==4 and len(stacked_next_frames)==4:\n",
        "\n",
        "            current_states = np.stack(list(stacked_current_frames))\n",
        "            next_states = np.stack(list(stacked_next_frames))\n",
        "\n",
        "            memory.push(current_states, action, next_states, reward, done)\n",
        "\n",
        "        current_frame = next_state\n",
        "        last_frame = state\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "\n",
        "        train_dqn(policy_net, target_net, memory, optimizer)\n",
        "\n",
        "        if t%50==0  and t!=0:\n",
        "            print(\"reward: \",episode_reward,f\"epsilon {epsilon:.2f}\")\n",
        "\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
        "    # Update target network\n",
        "    if episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    if episode_reward > best_reward and save_model is True:\n",
        "        best_reward = episode_reward\n",
        "        # Save the trained model\n",
        "        print(\"Saving model...\")\n",
        "        print(\"best rewards\",best_reward)\n",
        "        torch.save(policy_net.state_dict(), \"breakout.pth\")\n",
        "\n",
        "    rewards.append(episode_reward)\n",
        "    print(f\"Episode {episode+1}, Reward: {episode_reward}, Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Plot rewards\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(rewards)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('DQN Training on CartPole')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8fdd1ac",
      "metadata": {
        "id": "b8fdd1ac"
      },
      "outputs": [],
      "source": [
        "len(memory) >BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pCzxAyY-s1hs"
      },
      "id": "pCzxAyY-s1hs",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}